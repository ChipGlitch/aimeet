import openai
import vlc
from google.cloud import texttospeech
from moviepy.editor import *
from PIL import Image
import os
import requests

# VLC and API setup
os.environ["DYLD_LIBRARY_PATH"] = "/Applications/VLC.app/Contents/MacOS/lib/"
openai.api_key = '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'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/Zachc57/PycharmProjects/pythonProject/venv/myKey.json'
DEEP_AI_API_KEY = 'acca39c7-2279-4b6a-99b3-147a369e4f92'  # Replace with your DeepAI API key

# Initialize Google Text-to-Speech client
client = texttospeech.TextToSpeechClient()


def play_video(filename):
    player = vlc.MediaPlayer(filename)
    player.play()
    while player.is_playing():
        pass


def text_to_audio(text):
    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(language_code="en-US", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
    response_audio = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)

    audio_filename = 'response.mp3'
    with open(audio_filename, 'wb') as out:
        out.write(response_audio.audio_content)
    return audio_filename


def split_into_paragraphs(text):
    return [para.strip() for para in text.split("\n") if para.strip() != ""]


def generate_image_from_text(text):
    response = requests.post(
        "https://api.deepai.org/api/text2img",
        headers={"api-key": DEEP_AI_API_KEY},
        data={"text": text}
    )

    try:
        image_url = response.json()["output_url"]
        image_filename = "generated_image.jpg"
        image_content = requests.get(image_url).content
        with open(image_filename, 'wb') as file:
            file.write(image_content)
        return image_filename
    except KeyError:
        print("Error: Could not fetch the image URL from the API response.")
        return None


def generate_clip_from_text(text):
    paragraphs = split_into_paragraphs(text)
    audio_filename = text_to_audio(text)

    image_filenames = []
    for paragraph in paragraphs:
        img_filename = generate_image_from_text(paragraph)
        if img_filename:
            image_filenames.append(img_filename)

    audio_duration = AudioFileClip(audio_filename).duration
    clips_duration = len(paragraphs) * 2  # 2 seconds per paragraph
    duration_per_clip = audio_duration / len(paragraphs)

    clips = [ImageClip(filename).set_duration(duration_per_clip) for filename in image_filenames]
    concatenated_clip = concatenate_videoclips(clips, method="compose")
    audioclip = AudioFileClip(audio_filename)

    videoclip = concatenated_clip.set_audio(audioclip)

    for image_filename in image_filenames:
        if os.path.exists(image_filename):
            os.remove(image_filename)

    return videoclip


def generate_final_video(clips):
    final_clip = concatenate_videoclips(clips, method="compose")
    video_filename_input = input("Enter a name for the video (without extension): ")
    video_filename = f"{video_filename_input}.mp4"

    final_clip.fps = 24
    final_clip.write_videofile(video_filename, audio_codec='aac')

    return video_filename


def ai_meeting(topic, ai_count=3, leader="user"):
    roles = ["Sensing and Perception Expert", "Autonomous Navigation Specialist", "Collaborative Robotics Expert"]
    participants = [{"role": "assistant", "name": f"AI-{i}", "expertise": roles[i]} for i in range(ai_count)]
    messages = []

    if leader == "user":
        initial_message = input("You are leading the meeting. Start the conversation: ")
        messages.append({"role": "user", "content": initial_message})
    else:
        initial_message = f"{leader} initiates a conversation about {topic}."
        messages.append({"role": "assistant", "content": initial_message})

    print(f"\n[Meeting on {topic}]\n")

    clips = []

    for _ in range(5):  # This is where the loop happens.
        for participant in participants:
            expertise_prompt = f"As a {participant['expertise']}, considering previous insights, how can we further understand and expand upon the topic?"
            messages[-1]['content'] += f" {expertise_prompt}"

            try:
                response = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=messages)
                statement = response['choices'][0]['message']['content'].strip()

                video_clip = generate_clip_from_text(statement)
                clips.append(video_clip)

                print(f"{participant['name']} ({participant['expertise']}): {statement}\n")

                messages.append({"role": "assistant", "content": statement})
            except openai.error.OpenAIError as e:
                print(f"Error: {e}")
                return

        user_input = input("Your turn (type 'skip' to let the AIs continue, 'end' to conclude the meeting): ")

        if user_input.lower() == 'end':
            print("Meeting concluded by the user.")
            break
        elif user_input.lower() != 'skip':
            print(f"You: {user_input}\n")
            messages.append({"role": "user", "content": user_input})

    final_video_filename = generate_final_video(clips)
    play_video(final_video_filename)


if __name__ == "__main__":
    meeting_topic = input("Enter the topic for the meeting: ")
    leader_choice = input("Who should lead the meeting (user/AI-1/AI-2/etc.): ")
    ai_meeting(meeting_topic, leader=leader_choice)
